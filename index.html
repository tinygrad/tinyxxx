<!DOCTYPE html>
<html>
<head>
<title>
tinygrad: A simple and powerful neural network framework
</title>
<style>
  body {
    font-family:'Lucida Console', monospace
  }
</style>
<body>
<h1>we are the tiny corp</h1>

<p>We write and maintain <a href="https://github.com/geohot/tinygrad">tinygrad</a>, the fastest growing neural network framework (over 9000 GitHub stars)<p>

<p>It's extremely simple, and breaks down the most <a href="https://github.com/geohot/tinygrad/blob/master/examples/llama.py">complex</a> <a href="https://github.com/geohot/tinygrad/blob/master/examples/stable_diffusion.py">networks</a> into 4 <a href="https://github.com/geohot/tinygrad/blob/master/tinygrad/ops.py">OpTypes</a></p>

<li><b>UnaryOps</b> operate on one tensor and run elementwise. RELU, LOG, RECIPROCAL, etc...</li>
<li><b>BinaryOps</b> operate on two tensors and run elementwise to return one. ADD, MUL, etc...</li>
<li><b>ReduceOps</b> operate on one tensor and return a smaller tensor. SUM, MAX</li>
<li><b>MovementOps</b> operate on one tensor and move the data around, copy-free with <a href="https://github.com/geohot/tinygrad/blob/master/tinygrad/shape/shapetracker.py">ShapeTracker</a>. RESHAPE, PERMUTE, EXPAND, etc...</li>

<p>But how...where are your CONVs and MATMULs? Read the code to solve this mystery.</p>

<hr>
<h2>Work at the tiny corp</h2>

We are now funded and <b>hiring</b> software engineers. Full time and on site in San Diego. No remote. Very talented interns okay. Same as <a href="https://www.comma.ai/jobs">comma</a>.<br/><br/>

See <b>jobs that need doing</b> to judge if you might be a good fit.<br/><br/>

I'm supposed to pitch you emotionally on why you should work here.<br/>
Quit your job, move to San Diego, and work at the tiny corp.<br/>
It will give you that meaning you are searching for, and I'm only half kidding.<br/>
We are actually trying to make AI for everyone.<br/><br/>

<i>work@tinygrad.org</i> to apply.

<hr>

<h2>Jobs that need doing</h2>

If you come work here, this is a list to get started. Assumes some familiarity with the <a href="https://github.com/geohot/tinygrad">tinygrad</a> codebase.

<h3>Getting AMD on MLPerf</h3>
There's a <a href="https://github.com/users/geohot/projects/2">GitHub project</a> for this. ResNet is done, U-Net3D is almost done. Need RetinaNet, RNN-T, and BERT.<br/>
$200 per model bounty for writing these, learn more in the #mlperf-bounties channel on our <a href="https://discord.gg/ZjZadyC7PK">Discord</a><br/>

<h3>Fastest Stable Diffusion and LLaMA on M1</h3>
For LLaMA: Write the specializedjit and support int8.<br/>
For Stable Diffusion: Clean up to use BS=2, write winograd conv, and write kernel search. Target is 0.324 s/it.<br/>

<h3>Qualcomm DSP Support</h3>
Switch over comma's DSP based driver monitoring model to tinygrad.<br/>
A new architecture to port!</br>

<h3>Max out all the GEMMs</h3>
NVIDIA: Support local memory tensors. Some work in <a href="https://github.com/geohot/tinygrad/pull/557">this branch</a>.<br/>
Apple M1: Support using Tensor Cores and simdgroup_float8x8. Some work in <a href="https://github.com/geohot/tinygrad/pull/728">this branch</a>.<br/>
AMD: Write assembly backend for the linearizer<br/>

<h3>Multi GPU training</h3>
Trees, rings, and collnets oh my! AMD gets us P2P for free.

<h3>Kernel Search</h3>
There's a ton of ways to generate equivalent kernels, enumerate them and search them.<br/>
Maybe with ML based search running in tinygrad. Self improvement?

<h3>Kernel Combining (lazy.py)</h3>
ResNets don't fuse as nicely as they could because which reduce do you fuse the elementwise with?<br/>
BatchNorm at training and GroupNorm always is 4 kernels. Merge them!
<br/>

<hr>

<h2>The tinybox</h2>
<li>738 FP16 TFLOPS</li>
<li>144 GB GPU RAM</li>
<li>5.76 TB/s RAM bandwidth</li>
<li>30 GB/s model load bandwidth (big llama loads in around 4 seconds)</li>
<li>AMD EPYC CPU</li>
<li>1600W (one 120V outlet)</li>
<li>Runs 65B FP16 LLaMA out of the box (using tinygrad, subject to software development risks)</li>
<li><b>$15,000</b></li>
<br/>
<a href="https://buy.stripe.com/5kAaGL6lk9uX9nW144">Preorder a tinybox today! (just $100)</a><br/><br/>
(specs subject to change. all preorders fully refundable until your tinybox ships. price doesn't include shipping. estimated timeline 2-6 months)
<br/>
<hr>

<h2>FAQ:</h2>

<em>Is tinygrad used anywhere?</em><br/></br>
tinygrad is used in <a href="https://github.com/commaai/openpilot">openpilot</a> to run the driving model on the Snapdragon 845 GPU. It replaces <a href="https://developer.qualcomm.com/sites/default/files/docs/snpe/overview.html">SNPE</a>, is faster, supports loading onnx files, supports training, and allows for attention (SNPE only allows fixed weights).
<br/><br/><br/>

<em>Is tinygrad inference only?</em><br/></br>
No! It supports full forward and backward passes with autodiff. <a href="https://github.com/geohot/tinygrad/blob/master/tinygrad/mlops.py">This</a> is implemented at a level of abstraction higher than the accelerator specific code, so a tinygrad port gets you this for free.
<br/><br/><br/>

<em>How can I use tinygrad for my next ML project?</em><br/></br>
Follow the installation instructions on <a href="https://github.com/geohot/tinygrad">the tinygrad repo</a>. It has a similar API to PyTorch, yet simpler and more refined. Less stable though while tinygrad is in alpha, so be warned, though it's been fairly stable for a while.
<br/><br/><br/>

<em>When will tinygrad leave alpha?</em><br/></br>
When we can reproduce a common set of papers on 1 NVIDIA GPU 2x faster than PyTorch. We also want the speed to be good on the M1. ETA, Q2 next year.
<br/><br/><br/>

<em>How is tinygrad faster than PyTorch?</em><br/></br>
For most use cases it isn't yet, but it will be. It has three advantages:
<li>It compiles a custom kernel for every operation, allowing extreme shape specialization.</li>
<li>All tensors are lazy, so it can aggressively fuse operations.</li>
<li>The backend is 10x+ simpler, meaning optimizing one kernel makes everything fast.</li>
<br/><br/>

<em>How can the tiny corp work for me?</em><br/></br>

Email me, george@tinygrad.org. We are looking for contracts and sponsorships to improve various aspects of tinygrad.
<br/><br/><br/>

<em>How can I work for the tiny corp?</em><br/></br>

See <b>hiring</b> above. Contributions to <a href="https://github.com/geohot/tinygrad">tinygrad</a> on GitHub always welcome, and a good way to get hired.
<br/><br/><br/>

<em>Can I invest in the tiny corp?</em><br/></br>

Invest with your PRs.

<br/><br/><br/>

<em>What's the goal of the tiny corp?</em><br/></br>

To accelerate. We will commoditize the petaflop and enable AI for everyone.

