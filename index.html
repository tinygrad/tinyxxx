<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>
tinygrad: A simple and powerful neural network framework
</title>
<style>
  body {
    font-family:'Lucida Console', monospace;
    padding-bottom: 30px;
  }
  .logo {
    font-size: 60px;
    line-height: 36px;
  }
  table {
    border-collapse: collapse;
    text-align: center;
  }
  td {
    padding: 5px 20px;
    border: 1px solid #dddddd;
  }
  svg {
    width: 500px;
    padding: 30px;
  }
  .quiet {
    color: inherit;
  }
</style>
<body>
<svg viewBox="0 0 130 50" xmlns="http://www.w3.org/2000/svg">
  <!-- t -->
  <rect x="10" y="0" width="10" height="40" fill="#000000"/>
  <rect x="0" y="10" width="30" height="10" fill="#000000"/>
  <!-- i -->
  <rect x="40" y="0" width="10" height="10" fill="#000000"/>
  <rect x="40" y="20" width="10" height="20" fill="#000000"/>
  <!-- n -->
  <rect x="60" y="10" width="10" height="30" fill="#000000"/>
  <rect x="60" y="10" width="20" height="10" fill="#000000"/>
  <rect x="80" y="20" width="10" height="20" fill="#000000"/>
  <!-- y -->
  <rect x="100" y="10" width="10" height="20" fill="#000000"/>
  <rect x="100" y="20" width="30" height="10" fill="#000000"/>
  <rect x="120" y="10" width="10" height="30" fill="#000000"/>
  <rect x="100" y="40" width="20" height="10" fill="#000000"/>
</svg>
<p>We write and maintain <a href="https://github.com/tinygrad/tinygrad">tinygrad</a>, the fastest growing neural network framework (over 9000 GitHub stars)<p>

<p>It's extremely simple, and breaks down the most <a href="https://github.com/tinygrad/tinygrad/blob/master/examples/llama.py">complex</a> <a href="https://github.com/geohot/tinygrad/blob/master/examples/stable_diffusion.py">networks</a> into 3 <a href="https://github.com/geohot/tinygrad/blob/master/tinygrad/ops.py">OpTypes</a></p>

<li><b>ElementwiseOps</b> are UnaryOps, BinaryOps, and TernaryOps. They operate on 1-3 tensors and run elementwise. SQRT, LOG2, ADD, MUL, WHERE, etc...</li>
<li><b>ReduceOps</b> operate on one tensor and return a smaller tensor. SUM, MAX</li>
<li><b>MovementOps</b> are virtual ops that operate on one tensor and move the data around, copy-free with <a href="https://github.com/tinygrad/tinygrad/blob/master/tinygrad/shape/shapetracker.py">ShapeTracker</a>. RESHAPE, PERMUTE, EXPAND, etc...</li>

<p>But how...where are your CONVs and MATMULs? Read the code to solve this mystery.</p>

<hr>
<h2>Work at the tiny corp</h2>

We <a href="https://geohot.github.io/blog/jekyll/update/2023/05/24/the-tiny-corp-raised-5M.html">are now funded</a> and <b>hiring</b> full time software engineers. Very talented interns okay.<br/><br/>

See <a href="https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs/edit?usp=sharing">our bounty page</a> to judge if you might be a good fit. Bounties pay you while judging that fit.<br/><br/>

We are also hiring for operations and hardware, but if you haven't contributed to tinygrad your application won't be considered.
<hr>
<h2>We sell a computer called the tinybox. It comes in two colors</h2>
<table>
  <tr><td></td></td><td><b style="color:red">red</b></td><td><b style="color:green">green</b></td></tr>
  <tr><td>TFLOPS</td></td><td>738 FP16 TFLOPS</td><td>991 FP16 TFLOPS</td></tr>
  <tr><td>GPU RAM</td><td colspan="2">144 GB</td></tr>
  <tr><td>GPU RAM bandwidth</td><td>5760 GB/s</td><td>6050 GB/s</td></tr>
  <tr><td>GPU link bandwidth</td><td colspan="2">6x PCIe 4.0 x16 (64 GB/s)</td></tr>
  <tr><td>CPU</td><td colspan="2">32 core AMD EPYC</td></tr>
  <tr><td>System RAM</td><td colspan="2">128 GB</td></tr>
  <tr><td>System RAM bandwidth</td><td colspan="2">204.8 GB/s</td></tr>
  <tr><td>Disk size</td><td colspan="2">4 TB raid array + 1 TB boot</td></tr>
  <tr><td>Disk read bandwidth</td><td colspan="2"><a class="quiet" href="https://twitter.com/__tinygrad__/status/1747467257889116379">28.7 GB/s</a></td></tr>
  <tr><td>Networking</td><td colspan="2">Dual 1 GbE + open x16 OCP 3.0</td></tr>
  <tr><td>Noise</td><td colspan="2">&lt; 50 dB, 31 low speed fans</td></tr>
  <tr><td>Power Supply</td><td colspan="2">2x 1600W</td></tr>
  <tr><td>BMC</td><td colspan="2">AST2500</td></tr>
  <tr><td>Operating System</td><td colspan="2">Ubuntu 22.04</td></tr>
  <tr><td>Driver Quality</td></td><td>Mediocre</td><td>Great</td></tr>
  <tr><td>Price</td></td><td>$15,000</td><td>$25,000</td></tr>
</table>
<h3><a href="https://buy.stripe.com/5kAaGL6lk9uX9nW144">Preorder a tinybox today! (just $100)</a></h3>
(specs subject to change. all preorders fully refundable until your tinybox ships. price doesn't include shipping. estimated timeline 2-4 months)<br/>
<hr>

<h2>FAQ:</h2>

<em>How will my preorder turn into a tinybox?</em><br/></br>
You will be contacted by e-mail when we are ready to finalize your preorder. First boxes will ship in April 2024.
<br/><br/><br/>

<em>Is tinygrad used anywhere?</em><br/></br>
tinygrad is used in <a href="https://github.com/commaai/openpilot">openpilot</a> to run the driving model on the Snapdragon 845 GPU. It replaces <a href="https://developer.qualcomm.com/sites/default/files/docs/snpe/overview.html">SNPE</a>, is faster, supports loading onnx files, supports training, and allows for attention (SNPE only allows fixed weights).
<br/><br/><br/>

<em>Is tinygrad inference only?</em><br/></br>
No! It supports full forward and backward passes with autodiff. <a href="https://github.com/tinygrad/tinygrad/blob/master/tinygrad/mlops.py">This</a> is implemented at a level of abstraction higher than the accelerator specific code, so a tinygrad port gets you this for free.
<br/><br/><br/>

<em>How can I use tinygrad for my next ML project?</em><br/></br>
Follow the installation instructions on <a href="https://github.com/tinygrad/tinygrad">the tinygrad repo</a>. It has a similar API to PyTorch, yet simpler and more refined. Less stable though while tinygrad is in alpha, so be warned, though it's been fairly stable for a while.
<br/><br/><br/>

<em>When will tinygrad leave alpha?</em><br/></br>
When we can reproduce a common set of papers on 1 NVIDIA GPU 2x faster than PyTorch. We also want the speed to be good on the M1. ETA, Q2 next year.
<br/><br/><br/>

<em>How is tinygrad faster than PyTorch?</em><br/></br>
For most use cases it isn't yet, but it will be. It has three advantages:
<li>It compiles a custom kernel for every operation, allowing extreme shape specialization.</li>
<li>All tensors are lazy, so it can aggressively fuse operations.</li>
<li>The backend is 10x+ simpler, meaning optimizing one kernel makes everything fast.</li>
<br/><br/>

<em>Where is tinygrad development happening?</em><br/></br>

On GitHub and <a href="https://discord.com/invite/ZjZadyC7PK">on Discord</a>.
<br/><br/><br/>

<em>How can the tiny corp work for me?</em><br/></br>

Email me, george@tinygrad.org. We are looking for contracts and sponsorships to improve various aspects of tinygrad.
<br/><br/><br/>

<em>How can I work for the tiny corp?</em><br/></br>

See <b>hiring</b> above. Contributions to <a href="https://github.com/tinygrad/tinygrad">tinygrad</a> on GitHub always welcome, and a good way to get hired.
<br/><br/><br/>

<em>Can I invest in the tiny corp?</em><br/></br>

Invest with your PRs.

<br/><br/><br/>

<em>What's the goal of the tiny corp?</em><br/></br>

To accelerate. We will commoditize the petaflop and enable AI for everyone.

